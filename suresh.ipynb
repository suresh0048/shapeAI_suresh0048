In [7]:
from keras.datasets import mnist

data = mnist.load_data()
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
11493376/11490434 [==============================] - 0s 0us/step
In [8]:
((X_train, y_train), (X_test, y_test)) = data
In [9]:
X_train = X_train.reshape((X_train.shape[0],28*28)).astype('float32')
X_test = X_test.reshape((X_test.shape[0],28*28)).astype('float32')
In [10]:
X_train = X_train / 255
X_test = X_test / 255
In [11]:
from keras.utils import np_utils

print(y_test.shape)

y_train = np_utils.to_categorical(y_train)
y_test = np_utils.to_categorical(y_test)

num_classes = y_test.shape[1]
print(y_test.shape)
(10000,)
(10000, 10)
In [12]:
from keras.models import Sequential
from keras.layers import Dense
In [13]:
model = Sequential()
model.add(Dense(32, input_dim = 28*28, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(10, activation='softmax'))
In [14]:
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
In [15]:
model.summary()
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense (Dense)                (None, 32)                25120     
_________________________________________________________________
dense_1 (Dense)              (None, 64)                2112      
_________________________________________________________________
dense_2 (Dense)              (None, 10)                650       
=================================================================
Total params: 27,882
Trainable params: 27,882
Non-trainable params: 0
_________________________________________________________________
In [16]:
model.fit(X_train, y_train, epochs=10, batch_size=100)
Epoch 1/10
600/600 [==============================] - 2s 2ms/step - loss: 0.7603 - accuracy: 0.7808
Epoch 2/10
600/600 [==============================] - 1s 2ms/step - loss: 0.2056 - accuracy: 0.9408
Epoch 3/10
600/600 [==============================] - 1s 2ms/step - loss: 0.1602 - accuracy: 0.9527
Epoch 4/10
600/600 [==============================] - 1s 2ms/step - loss: 0.1360 - accuracy: 0.9596
Epoch 5/10
600/600 [==============================] - 1s 2ms/step - loss: 0.1129 - accuracy: 0.9673
Epoch 6/10
600/600 [==============================] - 1s 2ms/step - loss: 0.1008 - accuracy: 0.9679
Epoch 7/10
600/600 [==============================] - 1s 2ms/step - loss: 0.0884 - accuracy: 0.9726
Epoch 8/10
600/600 [==============================] - 2s 3ms/step - loss: 0.0811 - accuracy: 0.9744
Epoch 9/10
600/600 [==============================] - 1s 2ms/step - loss: 0.0744 - accuracy: 0.9771
Epoch 10/10
600/600 [==============================] - 2s 3ms/step - loss: 0.0666 - accuracy: 0.9796
Out[16]:
<tensorflow.python.keras.callbacks.History at 0x7f90f20823d0>
In [18]:
scores = model.evaluate(X_test, y_test)
print(scores)
313/313 [==============================] - 1s 1ms/step - loss: 0.1034 - accuracy: 0.9687
[0.10338249802589417, 0.9686999917030334] hi
